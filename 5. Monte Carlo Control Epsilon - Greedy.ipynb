{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "\n",
    "from collections import defaultdict\n",
    "env=gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Define Run Episode Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env,Q,epsilon,n_action):\n",
    "    #1.Initialize the first state and create lists for the rewards, actions and states during the simulation (As always)\n",
    "    state=env.reset()\n",
    "    rewards=[]\n",
    "    actions=[]\n",
    "    states=[]\n",
    "    is_done=False\n",
    "    #2.Initilize a loop that breaks when the player get a score>=21\n",
    "    while not is_done:\n",
    "    #3.Creates probabilities for action 0 and action 1. Epsilon goes up to 1 if we do not explore. If epsilon=0 we take all actions at random\n",
    "        probs=torch.ones(n_action)*epsilon/n_action\n",
    "    #4.We extract the best action out of the Q function. If Q function does not have this state it autmatically generates it with'state' as a key and creates a tensor (111,1111232)  as an item.  \n",
    "        best_action=torch.argmax(Q[state]).item()\n",
    "    #5.It gives the best action according to the Q function  a probability the largest weight.  \n",
    "        probs[best_action]+=1-epsilon\n",
    "    #6.Samples a number between 0,1 with probabilities discribed before\n",
    "        action=torch.multinomial(probs,1).item()\n",
    "    #7.Put all actions into a list\n",
    "        actions.append(action)\n",
    "        states.append(state)\n",
    "    #8.Now we finally take a step\n",
    "        state,reward,is_done,info=env.step(action)\n",
    "        rewards.append(reward)  \n",
    "    #9.Break the loop if the game is over\n",
    "        if is_done:\n",
    "            break\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: MC Control with Epsilon Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_epsilon_greedy(env,gamma,n_episode,epsilon):\n",
    "    #1.We define the action space\n",
    "    n_action=env.action_space.n\n",
    "    #2.This vector is going to capture the agreggate Gsum \n",
    "    G_sum=defaultdict(float)\n",
    "    #3.The N vector is going to capture the number of times I visit a particular state\n",
    "    N=defaultdict(int)\n",
    "    #4.Q Functions captures the payoffs of taking actions across every state fx. Q(0,a)=[Q(0,1),Q(0,0)]\n",
    "    Q=defaultdict(lambda:torch.empty(n_action))\n",
    "    #5.We start running multiple episodes\n",
    "    for episode in range(n_episode):\n",
    "        states_t, actions_t, rewards_t = run_episode(env,Q,epsilon,n_action)\n",
    "        return_t=0\n",
    "    #6.G Calculates the payoffs and states of an episode\n",
    "        G={}\n",
    "    #7.We loop starting from the end state,at the end of this loop we will have dictionary that captures state_action and returns visitied in the episode    \n",
    "        for state_t, action_t, reward_t in zip(states_t[::-1],actions_t[::-1], rewards_t[::-1]):\n",
    "            return_t=reward_t+gamma*return_t\n",
    "            G[(state_t,action_t)]=return_t\n",
    "    #8.'state_action' es el 'key' of the G dictionary and 'return_t' is the 'item'       \n",
    "        for state_action , return_t in G.items(): \n",
    "            state, action=state_action\n",
    "    #9.Si mis cartas son menores o iguales a 21        \n",
    "            if state[0]<=21:\n",
    "                G_sum[state_action]+= return_t\n",
    "                N[state_action] +=1\n",
    "    #10.El promedio vara cada estado accion queda guardado en su respectiva position en la funcion Q            \n",
    "                Q[state][action]=G_sum[state_action]\n",
    "    #11. When everything is done we create a policy with the best values of the Q functions for every state\n",
    "    policy={}\n",
    "    for state,actions in Q.items():\n",
    "        policy[state]=torch.argmax(actions).item()\n",
    "    return Q, policy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=1\n",
    "n_episode=500000\n",
    "epsilon=0.1\n",
    "\n",
    "optimal_q, optimal_policy=mc_control_epsilon_greedy(env,gamma,n_episode,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking wheter it really works!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episode=100000\n",
    "n_win_optimal=0\n",
    "n_lose_optimal=0\n",
    "for _ in range(n_episode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
